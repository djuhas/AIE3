{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.5-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from pymupdf)\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.5-cp311-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install PyPDF2 wandb scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: wandb in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: scikit-learn in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (5.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (2.5.0)\n",
      "Requirement already satisfied: setproctitle in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from wandb) (70.0.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/djuhas/anaconda3/envs/aie3week2d1/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l5rsfcul) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-firefly-15</strong> at: <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/l5rsfcul' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/l5rsfcul</a><br/> View project at: <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240610_143824-l5rsfcul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l5rsfcul). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/djuhas/beyondchatgptrepo/AIE3/AIE3/Week 2/Day 1/wandb/run-20240610_143948-21pl0fyd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/21pl0fyd' target=\"_blank\">dutiful-fog-16</a></strong> to <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/21pl0fyd' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/21pl0fyd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDFs...\n",
      "Extracted text from Text generation models.pdf\n",
      "Logged artifact for Text generation models.pdf\n",
      "Initializing RAG system...\n",
      "Ingesting extracted PDF texts into RAG system...\n",
      "Ingested document from Text generation models.pdf\n",
      "Updating document vectors with corpus: [' \\nText generation models  \\n \\nOpenAI\\'s text generation models (often called generative pre -trained transformers or \\nlarge language models) have been trained to understand natural language, code, \\nand images. The models provide  text outputs in response to their inputs. The text \\ninputs to these models are also referred to as \"prompts\". Designing a prompt is \\nessentially how you “program” a large language model model, usually by providing \\ninstructions or some examples of how to suc cessfully complete a task.  \\nUsing OpenAI\\'s text generation models, you can build applications to:  \\n• Draft documents  \\n• Write computer code  \\n• Answer questions about a knowledge base  \\n• Analyze texts  \\n• Give software a natural language interface  \\n• Tutor in a range of subjects  \\n• Translate languages  \\n• Simulate characters for games  \\n \\n \\nTry GPT -4o \\nTry out GPT -4o in the playground.  \\n \\nExplore GPT -4o with image inputs  \\nCheck out the vision guide for more detail.  \\n  \\nTo use one of these models via the OpenAI API, you’ll send a request to the Chat \\nCompletions API containing the inputs a nd your API key, and receive a response \\ncontaining the model’s output.  \\nYou can experiment with various models in the  chat playground . If you’re not sure \\nwhich model to use then try  gpt-4o if you need high intelligence or  gpt-3.5-turbo if you \\nneed the fastest speed and lowest cost.  \\n \\nChat Completions API  \\n \\nChat models take a list of messages as input and return a model -generated \\nmessage as output. Although the chat format is designed to make multi -turn \\nconversations easy, it’s just as useful for sin gle-turn tasks without any conversation.  \\nAn example Chat Completions API call looks like the following:  \\npython  \\nSelect librarypythonnode.jscurl  \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\nfrom  openai import  OpenAI  \\nclient = OpenAI()  \\n \\nresponse = client.chat.completions.create(  \\n  model= \"gpt -3.5-turbo\" , \\n  messages=[  \\n    {\"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, \\n    {\"role\" : \"user\" , \"content\" : \"Who won the world series in 2020?\" }, \\n    {\"role\" : \"assistant\" , \"content\" : \"The Los Angeles Dodgers won the World Series in 2020.\" }, \\n    {\"role\" : \"user\" , \"content\" : \"Where was it played?\" } \\n  ] \\n) \\nTo learn more, you can view the full  API reference documentation  for the Chat API.  The main input is the messages parameter. Messages must be an array of message \\nobjects, where eac h object has a role (either \"system\", \"user\", or \"assistant\") and \\ncontent. Conversations can be as short as one message or many back and forth \\nturns.  \\nTypically, a conversation is formatted with a system message first, followed by \\nalternating user and assis tant messages.  \\nThe system message helps set the behavior of the assistant. For example, you can \\nmodify the personality of the assistant or provide specific instructions about how it \\nshould behave throughout the conversation. However note that the system me ssage \\nis optional and the model’s behavior without a system message is likely to be similar \\nto using a generic message such as \"You are a helpful assistant.\"  \\nThe user messages provide requests or comments for the assistant to respond to. \\nAssistant messages  store previous assistant responses, but can also be written by \\nyou to give examples of desired behavior.  \\nIncluding conversation history is important when user instructions refer to prior \\nmessages. In the example above, the user’s final question of \"Where was it played?\" \\nonly makes sense in the context of the prior messages about the World Series of \\n2020. Because the models have no memory of past requests, all relevant information \\nmust be supplied as part of the conversation history in each request. If a \\nconversation cannot fit within the model’s token limit, it will need to be  shortened  in \\nsome way.  \\nTo mimic the effect seen in ChatGPT where the text is returned iteratively, set \\nthe stream  parameter to true.  \\n \\nChat Completions response format  \\n \\nAn example Chat Completions API response looks as follows:  \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n{ \\n  \"choices\": [  \\n    { \\n      \"finish_reason\": \"stop\",  \\n      \"index\": 0,  \\n      \"message\": {  \\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",  \\n        \"role\": \"assistant\"  \\n      }, \\n      \"logprobs\": null  \\n    } \\n  ], \\n  \"created\": 1677664795,  \\n  \"id\": \"chatcmpl -7QyqpwdfhqwajicIEznoc6Q47XAyW\",  \\n  \"model\": \"gpt -3.5-turbo -0613\",  \\n  \"object\": \"chat.completion\",  \\n  \"usage\": {  \\n    \"completion_tokens\": 17,  \\n    \"prompt_tokens\": 57,  \\n    \"total_tokens\": 74  \\n  } \\n} \\nThe assistant’s reply can be extracted with:  \\npython  \\nSelect librarypythonnode.js  \\ncompletion.choices [0].message.content  \\nEvery response will include a  finish_reason . The possible values for  finish_reason  are: • stop: API returned complete message, or a message terminated by one of the \\nstop sequences provided via the  stop parameter  \\n• length : Incomplete model output due to  max_tokens  parameter or token limit  \\n• function_call : The model decided to call a function  \\n• content_filter : Omitted content due to a flag from our content filters  \\n• null: API response still in progress or incomplete  \\nDepending on input parameters, the model response may include different \\ninformation.  \\n \\nJSON mode  \\n \\nA common way to use Chat Completions is to instruct the model to always return a \\nJSON object that makes sense for your use case , by specifying this in the system \\nmessage. While this does work in some cases, occasionally the models may \\ngenerate output that does not parse to valid JSON objects.  \\nTo prevent these errors and improve model performance, when using  gpt-4o, gpt-4-\\nturbo , or gpt-3.5-turbo, you can set  response_format  to { \"type\": \"json_object\" }  to enable \\nJSON mode. When JSON mode is ena bled, the model is constrained to only \\ngenerate strings that parse into valid JSON object.  \\nImportant notes:  \\n• When using JSON mode,  always  instruct the model to produce JSON via \\nsome message in the conversation, for example via your system message. If \\nyou do n\\'t include an explicit instruction to generate JSON, the model may \\ngenerate an unending stream of whitespace and the request may run \\ncontinually until it reaches the token limit. To help ensure you don\\'t forget, the \\nAPI will throw an error if the string  \"JSON\"  does not appear somewhere in the \\ncontext.  \\n• The JSON in the message the model returns may be partial (i.e. cut off) \\nif finish_reason  is length , which indicates the generation exceeded  max_tokens  or \\nthe conversation exceeded the token limit. To guard ag ainst this, \\ncheck  finish_reason  before parsing the response.  \\n• JSON mode will not guarantee the output matches any specific schema, only \\nthat it is valid and parses without errors.  \\npython  \\nSelect librarypythonnode.jscurl  \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 9 \\n10 \\n11 \\n12 \\nfrom  openai import  OpenAI  \\nclient = OpenAI()  \\n \\nresponse = client.chat.completions.create(  \\n  model= \"gpt -3.5-turbo -0125\" , \\n  response_format={ \"type\" : \"json_object\"  }, \\n  messages=[  \\n    {\"role\" : \"system\" , \"content\" : \"You are a helpful assistant designed to output JSO N.\"}, \\n    {\"role\" : \"user\" , \"content\" : \"Who won the world series in 2020?\" } \\n  ] \\n) \\nprint (response.choices[ 0].message.content)  \\nIn this example, the response includes a JSON object that looks something like the \\nfollowing:  \\n\"content\" : \"{\\\\\"winner \\\\\": \\\\\"Los Angeles Dodgers \\\\\"}\"` \\nNote that JSON mode is always enabled when the model is generating arguments \\nas part of  function calling . \\n \\nReproducible outputs   \\nBeta  \\n \\nChat Completions are non -deterministic by default (which means model outputs may \\ndiffer from request to request). That being said, we offer some control towards \\ndeterministic outputs by giving you access to the  seed  parameter and \\nthe system_fingerprint  response field.  \\nTo receive (mostly) deterministic outputs across API calls, you can:  \\n• Set the  seed parameter to any integer of your choice and use the same value \\nacross requests you\\'d like deterministic outputs for.  \\n• Ensure all other parameters (like  prompt  or temperature ) are the exact same \\nacross requests.  \\nSometimes, determinism may be impacted due t o necessary changes OpenAI \\nmakes to model configurations on our end. To help you keep track of these changes, \\nwe expose the  system_fingerprint  field. If this value is different, you may see \\ndifferent outputs due to changes we\\'ve made on our systems.  \\n \\nDeterministic outputs  \\nExplore the new seed parameter in the OpenAI cookbook   \\n \\nManaging tokens  \\n \\nLanguage models read and write text in chunks called tokens. In English, a token \\ncan be as short as one character o r as long as one word (e.g.,  a or apple), and in \\nsome languages tokens can be even shorter than one character or even longer than \\none word.  \\nFor example, the string  \"ChatGPT is great!\"  is encoded into six tokens:  [\"Chat\", \"G\", \"PT\", \" \\nis\", \" great\", \"!\"] . \\nThe total number of tokens in an API call affects:  \\n• How much your API call costs, as you pay per token  \\n• How long your API call takes, as writing more tokens takes more time  \\n• Whether your API call works at all, as total tokens must be below the model’s \\nmaximum limit (4097 tokens for  gpt-3.5-turbo ) \\nBoth input and output tokens count toward these quantities. For example, if your API \\ncall used 10 tokens in the message input and you received 20 tokens in the \\nmessage output, you would be billed for 30 tokens. Note however that for some \\nmodels the price p er token is different for tokens in the input vs. the output (see \\nthe pricing  page for more information).  \\nTo see how many tokens are used by an API call, check the  usage  field in the API \\nresponse (e.g.,  response[\\'usage\\'][\\'total_tokens\\'] ). \\nChat models like  gpt-3.5-turbo  and gpt-4-turbo -preview  use tokens in the same way as \\nthe models available in the completion s API, but because of their message -based \\nformatting, it\\'s more difficult to count how many tokens will be used by a \\nconversation.  \\nDEEP DIVE  \\nCounting tokens for chat API calls  \\nTo see how many tokens are in a text string without making an API call, use \\nOpen AI’s tiktoken  Python library. Example code can be found in the OpenAI \\nCookbook’s guide on  how to count tokens with tiktoken . \\nEach message passed to the API consumes the number of tokens in the content, \\nrole, and other fields, plus a few extra for behind -the-scenes formatting. This m ay \\nchange slightly in the future.  \\nIf a conversation has too many tokens to fit within a model’s maximum limit (e.g., \\nmore than 4097 tokens for  gpt-3.5-turbo  or more than 128k tokens for  gpt-4o), you will \\nhave to truncate, omit, or otherwise shrink your tex t until it fits. Beware that if a \\nmessage is removed from the messages input, the model will lose all knowledge of \\nit. \\nNote that very long conversations are more likely to receive incomplete replies. For \\nexample, a  gpt-3.5-turbo conversation that is 4090 t okens long will have its reply cut \\noff after just 6 tokens.  \\n Parameter details  \\n \\n \\nFrequency and presence penalties  \\n \\nThe frequency and presence penalties found in the  Chat Completions \\nAPI and Legacy Completions API  can be used to reduce the likelihood of sampling \\nrepetitive sequences of tokens.  \\nDEEP DIVE  \\nPenalties behind the scenes  \\nReasonable values for the penalty coefficients are around 0.1 to 1 if the aim  is to just \\nreduce repetitive samples somewhat. If the aim is to strongly suppress repetition, \\nthen one can increase the coefficients up to 2, but this can noticeably degrade the \\nquality of samples. Negative values can be used to increase the likelihood of  \\nrepetition.  \\n \\nToken log probabilities  \\n \\nThe logprobs  parameter found in the  Chat Completions API  and Legacy Completions \\nAPI, when requested, provides the log probabilities of each output t oken, and a \\nlimited number of the most likely tokens at each token position alongside their log \\nprobabilities. This can be useful in some cases to assess the confidence of the \\nmodel in its output, or to examine alternative responses the model might have gi ven. \\n \\nCompletions API   \\nLegacy  \\n \\nThe completions API endpoint received its final update in July 2023 and has a \\ndifferent interface than the new chat completions endpoint. Instead of the input being \\na list of messages, the input is a freeform text string called a  prompt . \\nAn example legacy Completions API call looks like the following:  \\npython  \\nSelect librarypythonnode.js  \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nfrom  openai import  OpenAI  \\nclient = OpenAI()  \\n response = client.completions.create(  \\n  model= \"gpt -3.5-turbo -instruct\" , \\n  prompt= \"Write a tagline for an ice cream shop.\"  \\n) \\nSee the full  API reference documentation  to learn more.  \\n \\nInserting text  \\n \\nThe completions endpoint also supports inserting text by providing a  suffix  in addition \\nto the standard prompt which is treated as a prefix. This need naturally arises when \\nwriting long -form text, transitioning between paragraphs, following an outline, or \\nguiding the model towards an ending. This also works on code, and can be used to \\ninsert in the middle of a function or file.  \\nDEEP DIVE  \\nInserting text  \\n \\nCompletions response format  \\n \\nAn example completions API response looks as follows:  \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n{   \"choices\": [  \\n    { \\n      \"finish_reason\": \"length\",  \\n      \"index\": 0,  \\n      \"logprobs\": null,  \\n      \"text\": \" \\\\n\\\\n\\\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"  \\n    } \\n  ], \\n  \"created\": 1683130927,  \\n  \"id\": \"cmpl -7C9Wxi9Du4j1lQ jdjhxBlO22M61LD\",  \\n  \"model\": \"gpt -3.5-turbo -instruct\",  \\n  \"object\": \"text_completion\",  \\n  \"usage\": {  \\n    \"completion_tokens\": 16,  \\n    \"prompt_tokens\": 10,  \\n    \"total_tokens\": 26  \\n  } \\n} \\nIn Python, the output can be extracted with  response[\\'choices\\'][0][\\'text\\'] . \\nThe response format is similar to the response format of the Chat Completions API.  \\n \\nChat Completions vs. Completions  \\n \\nThe Chat Completions format can be made similar to the completions format by \\nconstructing a request using a single user message. For example, one can translate \\nfrom English to Fr ench with the following completions prompt:  \\nTranslate the following English text to French: \"{text}\"  \\nAnd an equivalent chat prompt would be:  \\n[{\"role\": \"user\", \"content\": \\'Translate the following English text to French: \"{text}\"\\'}]  \\nLikewise, the completions  API can be used to simulate a chat between a user and an \\nassistant by formatting the input  accordingly . \\nThe difference between these APIs is the underlying models that are available in \\neach. The chat completions API is the interface to our most capable model ( gpt-4o), \\nand our most cost effective model ( gpt-3.5-turbo). \\n \\nPrompt engineering  \\n An awareness of the best practices for working with OpenAI models can make a \\nsignificant difference in application performance. The failure modes that each exhibit \\nand the ways of working around or correcting those failure modes are not always \\nintuitive. T here is an entire field related to working with language models which has \\ncome to be known as \"prompt engineering\", but as the field has progressed its scope \\nhas outgrown merely engineering the prompt into engineering systems that use \\nmodel queries as comp onents. To learn more, read our guide on  prompt \\nengineering  which covers methods to improve model reasoning, reduce the \\nlikelihood of model hallucin ations, and more. You can also find many useful \\nresources including code samples in the  OpenAI Cookbook . \\n \\nFAQ  \\n \\n \\nWhich model should I use?  \\n \\nWe generally recommend that you default to using either  gpt-4o, gpt-4-turbo , or gpt-3.5-\\nturbo . If your use case requires high intelligence or reasoning about images as well \\nas text, we recommend you evaluate both  gpt-4o and gpt-4-turbo  (although they have \\nvery similar intelligence, note that  gpt-4o is both faster and cheaper). If your use case \\nrequires the fastest speed and lowest cost, we recommend  gpt-3.5-turbo  since it is \\noptimized for these aspects.  \\ngpt-4o and gpt-4-turbo are also less likely than  gpt-3.5-turbo to make up information, a \\nbehavior known as \"hallucination\". Finally,  gpt-4o and gpt-4-turbo  have a context \\nwindow that supports up to 128,000 tokens compared to 4,096 tokens for  gpt-3.5-\\nturbo , meaning they can reason over much more information at  once.  \\nWe recommend experimenting in the  playground  to investigate which models \\nprovide the best price performance trade -off for your usage. A com mon design \\npattern is to use several distinct query types which are each dispatched to the model \\nappropriate to handle them.  \\n \\nHow should I set the temperature parameter?  \\n \\nLower values for temperature result in more consistent outputs (e.g. 0.2), while \\nhigher values generate more diverse and creative results (e.g. 1.0). Select a \\ntemperature value based on the desired trade -off between coherence and creativity \\nfor your specif ic application. The temperature can range is from 0 to 2.  \\n \\nIs fine -tuning available for the latest models?  \\n \\nSee the  fine-tuning guide  for the latest information on which models are available for \\nfine-tuning and ho w to get started.  \\n \\nDo you store the data that is passed into the API?   \\nAs of March 1st, 2023, we re tain your API data for 30 days but no longer use your \\ndata sent via the API to improve our models. Learn more in our  data usage policy . \\nSome endpoints o ffer zero retention . \\n \\nHow can I make my application more safe?  \\n \\nIf you want to add a moderation layer to the outputs of the Chat API, you can follow \\nour moderation guide  to prevent content that violates OpenAI’s usage policies from \\nbeing shown. We also encourage you to read our  safety guide  for more information \\non how to build safer systems.  \\n \\nShould I use ChatGPT or the API?  \\n \\nChatGPT  offers a chat interface for our models and a range of built -in features such \\nas integrated browsing, code execution, plugins, and more. By contrast, using \\nOpenAI’s API provides more flexibility but requires that y ou write code or send the \\nrequests to our models programmatically.  \\n ']\n",
      "Updated document vectors\n",
      "Logging the number of documents ingested...\n",
      "Performing retrieval for query: What is text generation?\n",
      "Retrieved best matching document\n",
      "Best document source: Text generation models.pdf\n",
      "Best document text:  \n",
      "Text generation models  \n",
      " \n",
      "OpenAI's text generation models (often called generative pre -trained transformers or \n",
      "large language models) have been trained to understand natural language, code, \n",
      "and images. The models provide  text outputs in response to their inputs. The text \n",
      "inputs to these models are also referred to as \"prompts\". Designing a prompt is \n",
      "essentially how you “program” a large language model model, usually by providing \n",
      "instructions or some examples of how to suc cessfully com\n",
      "Logging query and best document source to WandB...\n",
      "Ending WandB run...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>documents_ingested</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_document_source</td><td>Text generation mode...</td></tr><tr><td>documents_ingested</td><td>1</td></tr><tr><td>query</td><td>What is text generat...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-fog-16</strong> at: <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/21pl0fyd' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3/runs/21pl0fyd</a><br/> View project at: <a href='https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/tehnickapodrska/Visibility%20Example%20-%20AIE3</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240610_143948-21pl0fyd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install PyPDF2 wandb scikit-learn\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import wandb\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize WandB\n",
    "print(\"Initializing WandB...\")\n",
    "wandb.init(project=\"Visibility Example - AIE3\", entity=\"tehnickapodrska\")\n",
    "\n",
    "# Function to extract text from all PDFs in a folder using PyPDF2\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text()\n",
    "                if text.strip():  # Check if the text is not empty\n",
    "                    pdf_texts.append({\"filename\": filename, \"text\": text})\n",
    "                    print(f\"Extracted text from {filename}\")\n",
    "                else:\n",
    "                    print(f\"No text extracted from {filename}\")\n",
    "    return pdf_texts\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z0-9_\\-.]', '_', filename)\n",
    "\n",
    "# Check if there are any PDF files in the subfolder\n",
    "pdf_folder = 'PDF'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "if not pdf_files:\n",
    "    print(f\"No PDF files found in the folder '{pdf_folder}'. Please add some PDF files and try again.\")\n",
    "else:\n",
    "    # Extract text from all PDFs in the PDF folder\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    pdf_texts = extract_text_from_pdfs(pdf_folder)\n",
    "\n",
    "    # Check if any text was extracted\n",
    "    if not pdf_texts:\n",
    "        print(\"No text extracted from any PDFs. Exiting.\")\n",
    "    else:\n",
    "        # Save extracted text to individual files and log as artifacts\n",
    "        for pdf in pdf_texts:\n",
    "            sanitized_filename = sanitize_filename(pdf['filename'])\n",
    "            text_filename = f\"extracted_{sanitized_filename}.txt\"\n",
    "            with open(text_filename, \"w\") as text_file:\n",
    "                text_file.write(pdf['text'])\n",
    "            pdf_artifact = wandb.Artifact(sanitized_filename, type=\"dataset\")\n",
    "            pdf_artifact.add_file(text_filename)\n",
    "            wandb.log_artifact(pdf_artifact)\n",
    "            print(f\"Logged artifact for {pdf['filename']}\")\n",
    "\n",
    "        # Define the RAG system class\n",
    "        class RAGSystem:\n",
    "            def __init__(self):\n",
    "                self.documents = []\n",
    "                self.vectorizer = TfidfVectorizer()\n",
    "                self.doc_vectors = None\n",
    "            \n",
    "            def ingest_document(self, text, source=\"unknown\"):\n",
    "                if text.strip():  # Check if the text is not empty\n",
    "                    self.documents.append({\"text\": text, \"source\": source})\n",
    "                    print(f\"Ingested document from {source}\")\n",
    "                    self._update_vectors()\n",
    "                else:\n",
    "                    print(f\"Skipped empty document from {source}\")\n",
    "            \n",
    "            def _update_vectors(self):\n",
    "                corpus = [doc[\"text\"] for doc in self.documents]\n",
    "                print(f\"Updating document vectors with corpus: {corpus}\")\n",
    "                if corpus:\n",
    "                    self.doc_vectors = self.vectorizer.fit_transform(corpus)\n",
    "                    print(\"Updated document vectors\")\n",
    "                else:\n",
    "                    print(\"No documents to update vectors\")\n",
    "            \n",
    "            def ingest_pdf_texts(self, pdf_texts):\n",
    "                for pdf in pdf_texts:\n",
    "                    self.ingest_document(pdf['text'], source=pdf['filename'])\n",
    "            \n",
    "            def retrieve(self, query):\n",
    "                try:\n",
    "                    query_vector = self.vectorizer.transform([query])\n",
    "                    similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()\n",
    "                    best_match_index = similarities.argmax()\n",
    "                    print(\"Retrieved best matching document\")\n",
    "                    return self.documents[best_match_index]\n",
    "                except NotFittedError as e:\n",
    "                    print(\"Error during retrieval: The TF-IDF vectorizer is not fitted. Ensure that the document vectors are updated properly.\")\n",
    "                    raise e\n",
    "\n",
    "        # Initialize the RAG system\n",
    "        print(\"Initializing RAG system...\")\n",
    "        rag_system = RAGSystem()\n",
    "\n",
    "        # Ingest the extracted PDF texts\n",
    "        print(\"Ingesting extracted PDF texts into RAG system...\")\n",
    "        rag_system.ingest_pdf_texts(pdf_texts)\n",
    "\n",
    "        # Log the number of documents ingested\n",
    "        print(\"Logging the number of documents ingested...\")\n",
    "        wandb.log({\"documents_ingested\": len(rag_system.documents)})\n",
    "\n",
    "        # Perform a retrieval and log the results\n",
    "        query = \"What is text generation?\"\n",
    "        print(f\"Performing retrieval for query: {query}\")\n",
    "        try:\n",
    "            best_document = rag_system.retrieve(query)\n",
    "            print(f\"Best document source: {best_document['source']}\")\n",
    "            print(f\"Best document text: {best_document['text'][:500]}\")  # Print the first 500 characters\n",
    "            # Log the query and the best document source to WandB\n",
    "            print(\"Logging query and best document source to WandB...\")\n",
    "            wandb.log({\"query\": query, \"best_document_source\": best_document['source']})\n",
    "        except NotFittedError:\n",
    "            print(\"Retrieval failed due to vectorizer fitting issue.\")\n",
    "\n",
    "        # End the WandB run\n",
    "        print(\"Ending WandB run...\")\n",
    "        wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie3week2d1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
